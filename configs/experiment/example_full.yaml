# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
    - override /trainer: null  # override trainer to null so it's not loaded from main config defaults...
    - override /model: null
    - override /datamodule: null
    - override /callbacks: null
    - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: 12345

trainer:
    _target_: pytorch_lightning.Trainer
    gpus: 1
    min_epochs: 40
    max_epochs: 200
    gradient_clip_val: 0.5
    accumulate_grad_batches: 2
    weights_summary: "top"
    # resume_from_checkpoint: ${work_dir}/last.ckpt

model:
    _target_: src.models.audio_model.AudioLitModel
    lr: 0.001
    weight_decay: 0.00005
    architecture: resnext50_32x4d
    classes: ${classes}
    sample_rate: ${sample_rate}
    duration: ${duration}
    n_mels: 64
    n_fft: 1024 
    hop_len: 512
    top_db: 80
    n_mfcc: 64


datamodule:
    _target_: src.datamodules.audio_datamodule.AudioDataModule
    data_dir: ${data_dir}  
    sample_rate: ${sample_rate}
    duration: ${duration}
    classes: ${classes}
    batch_size: 10
    train_val_test_split: [0.8, 0.1, 0.1]
    num_workers: 5
    pin_memory: False

callbacks:
    model_checkpoint:
        _target_: pytorch_lightning.callbacks.ModelCheckpoint
        monitor: "val/loss_best"
        save_top_k: 2
        save_last: True
        mode: "min"
        dirpath: 'checkpoints/'
        filename: 'sample-audio-{epoch:02d}'
    early_stopping:
        _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: "val/loss_best"
        patience: 10
        mode: "min"
    watch_model_with_wandb:
        _target_: src.callbacks.wandb_callbacks.WatchModelWithWandb
        log: "all"
        log_freq: 100

logger:
    wandb:
        _target_: pytorch_lightning.loggers.wandb.WandbLogger
        project: "endomic"
        # entity: ""  # set to name of your wandb team or just remove it
        # offline: False  # set True to store all logs only locally
        job_type: "train"
        group: ""
        save_dir: "."

hydra:
    # output paths for hydra logs
    run:
        dir: logs/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    sweep:
        dir: logs/multiruns/${now:%Y-%m-%d_%H-%M-%S}
        subdir: ${hydra.job.num}

    # you can set here environment variables that are universal for all users
    # for system specific variables (like data paths) it's better to use .env file!
    job:
        env_set:
            HYDRA_FULL_ERROR: "1"
